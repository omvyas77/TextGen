{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f74e1",
   "metadata": {
    "id": "8c8f74e1",
    "outputId": "173a1ab2-8a6d-41cd-e9bc-55d85d99e484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 11:05:01--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'input.txt.1'\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M   332KB/s    in 3.3s    \n",
      "\n",
      "2023-03-23 11:05:07 (332 KB/s) - 'input.txt.1' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "   !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d70220f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the .tokens file for reading\n",
    "with open(r'C:\\Users\\newom\\Desktop\\wikitext-103\\wiki.train.tokens', 'r',encoding= 'utf-8') as f:\n",
    "    # Read the contents of the file into a string variable\n",
    "    tokens_t = f.read()\n",
    "with open('file_train.txt', 'w',encoding= 'utf-8') as f:\n",
    "    # Write the tokens to the file\n",
    "    f.write(tokens_t)\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(r'C:\\Users\\newom\\Desktop\\wikitext-103\\wiki.test.tokens', 'r',encoding= 'utf-8') as f:\n",
    "    # Read the contents of the file into a string variable\n",
    "    tokens_te = f.read()\n",
    "with open('file_test.txt', 'w',encoding= 'utf-8') as f:\n",
    "    # Write the tokens to the file\n",
    "    f.write(tokens_te)\n",
    "    \n",
    "with open(r'C:\\Users\\newom\\Desktop\\wikitext-103\\wiki.valid.tokens', 'r',encoding= 'utf-8') as f:\n",
    "    # Read the contents of the file into a string variable\n",
    "    tokens_v = f.read()\n",
    "with open('file_valid.txt', 'w',encoding= 'utf-8') as f:\n",
    "    # Write the tokens to the file\n",
    "    f.write(tokens_v)\n",
    "\n",
    "# Print the contents of the file\n",
    "tokens_t[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45034d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f27eab12",
   "metadata": {
    "id": "f27eab12",
    "outputId": "8ab56579-4360-4b9b-d810-cef7dd35e807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  538360726\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739b87d7",
   "metadata": {
    "id": "739b87d7",
    "outputId": "d46df01e-ef4c-482f-b678-311b05c11262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomer\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d683499",
   "metadata": {
    "id": "3d683499",
    "outputId": "37c6520f-9e6a-472a-9ce1-4e899e85a43e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¥§©«­®¯°±²³´µ¶·¹º»¼½¾¿ÀÁÂÄÅÆÇÈÉÌÍÎÑÓÔÖ×ØÚÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿĀāăąĆćČčĎĐđēėęěğġĦħĩĪīİıķĽľŁłńņňŋŌōŏőŒœŘřŚśŞşŠšŢţťũūŭůųŵŷŹźŻżŽžƏƒơưǂǎǐǒǔǫȘșȚțɐɑɒɓɔɕɖɗəɛɟɡɢɣɦɧɨɪɬɯɲɴɵɸɻɾʀʁʂʃʇʈʊʋʌʍʎʒʔʕʘʝʟʰʲʷʻʼʾʿˀˁˈˌː˔˘˚˞ˠˤ˥˦˧˨˩̧̝̞̟̠̣̤̥̩̪̯̰̱̲̺͍̀́̂̃̄̆̈̊̌̍̐̽̚͘͜͡ΑΒΓΔΕΗΘΙΚΛΜΝΞΟΠΣΤΦΧΨΩάέήίαβγδεζηθικλμνξοπρςστυφχψωόύώϕЈАБВГДЕЗИЙКМНОПРСТЧШЯабвгдежзийклмнопрстухцчшъыьэюяёіјљўԿՀՄՍաբեթիկհղյնշոպսվտրցւքֵֶּ֤֫אבגדוחיכלםמןנסעצרשת،ءأإابةتثجحخدرزسشصطعغفقكلمنهويیंअआउकगटडतदनपबभमयरलशषसह़ािीुेो्।॥কতনবমল়াি্கனள்్್്්กขงจชฐณตทนปพมยรลวะัาิีู฿เแ็่้์་།ငဆနရ္်გდვზიკორსუცძწხჯ჻፡ᛃᛋᛟតនពមរសហីុោះ់្᷉ḍḑḤḥḩḷḻṃṅṇṉṛṟṢṣṭṯạảấầẩẫậắằẵặẹếềểễệịỌọỏốồổỗộớờởợụủứừửữựỳỵỹἀἄἈἐἝἡἨἰἱἵἶἸὁὌὐὑὡὰὲὴὶὸὺὼᾶῆῖῦῬῶῷ​‌‍‐‑‒–—―‘’‚“”„†‡•…‰′″※‼‿⁄⁊⁡⁺₁₂₃₡₣₤₦₨₩₫€₱₹℃ℓ№™⅓⅔⅛⅜⅝⅞←↑→↓↔↗↦↪⇄⇌∀∂∆∈∑−∕∖∗∘√∝∞∩∪∴∼≈≠≡≢≤≥≪⊂⊕⊗⊙⊥⋅⋯⌊⌋①②④█▲★☆☉♀♠♣♥♦♩♪♭♮♯⟦⟧⟨⟩⩽ⴷ、。〈〉「」『』〜あいぅうおかがきくぐこさしじすずせたちつてとどなのはばふへぽまみめよらりるんァアィイウェエォオカガキギクグケコゴサザシジスズセゼソタダチッツテデトドナニノハバパヒビフブプベペボポマミムメモャヤュユョラリルレロワンヴ・ー一三上下不丑世个久之乙二五亦京人介仙令伝住佛作併侠個催像光内写冴净凪分列刘初判別前剛剣劇動勝北匹千卒南博厘双台号同名君呼哈四回国國土坂場境士外大天太女姉婷子字季孩守宋宗寔寺小少尾巨廷式弘张張彼後心思愛憲戦房所撃攻新方日旦早星春時書月朝木本李杜束条東松枚條植椎楽様機正武歩死殻氏民気氣水氷氾法活流浅海溜瀬火無版特王瓦生用田町界畫皮真礮神秋積空章箇節米約紋綾緑編群羽耕聖肖膺芝花草莊蛍街裁装西見語説議词谷路軍転輝逆進道遠邪部野量金銀銖錬隊隴隻雄集雪零靈青韓風鬼魂魄魔鼓거루마막말사인전지짓투하ﬁ﻿！＆（），－：＝？～｢･￥\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876a15e1",
   "metadata": {
    "id": "876a15e1",
    "outputId": "f5f75dd8-96d7-43ae-ac7b-a9bcc49bedf6"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import unicodedata\n",
    "# function to remove accented characters\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "# call function\n",
    "remove_accented_chars(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19d7182a",
   "metadata": {
    "id": "19d7182a",
    "outputId": "8804fcb1-2d2a-4cdf-a51d-eb4fe22ba88a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n  Valkyria Chronicles III  \\n \\n Senj no Valkyria 3 : unk Chronicles  Japanese : 3 , lit . Valkyria of the Battlefield 3  , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role  playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real  time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" unk Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomer'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import re\n",
    "# function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # define the pattern to keep\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pat, '', text[:1000])\n",
    " \n",
    "# call function\n",
    "remove_special_characters(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20b9a865",
   "metadata": {
    "id": "20b9a865",
    "outputId": "c22f798f-5528-48e9-f1d5-a9e77135aee3"
   },
   "outputs": [],
   "source": [
    "re.sub('\\s+',' ',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b56ee78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"t and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to t\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a81c82d",
   "metadata": {
    "id": "0a81c82d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b6acc1",
   "metadata": {
    "id": "f4b6acc1",
    "outputId": "3a5749dc-2870-47e1-82dc-43966e3ef83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 74, 74, 1, 85, 73, 70, 83, 70]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s:[stoi[c] for c in s]\n",
    "decode = lambda l:''.join([itos[i] for i in l])\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f255ae4e",
   "metadata": {
    "id": "f255ae4e",
    "outputId": "2c274ba6-c05c-4dc1-e5e3-bd7a7d1a539c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([538360726]) torch.int64\n",
      "tensor([   1,    0,    1,   30,    1,   55,   66,   77,   76,   90,   83,   74,\n",
      "          66,    1,   36,   73,   83,   80,   79,   74,   68,   77,   70,   84,\n",
      "           1,   42,   42,   42,    1,   30,    1,    0,    1,    0,    1,   52,\n",
      "          70,   79,   75,  214,    1,   79,   80,    1,   55,   66,   77,   76,\n",
      "          90,   83,   74,   66,    1,   20,    1,   27,    1,   29,   86,   79,\n",
      "          76,   31,    1,   36,   73,   83,   80,   79,   74,   68,   77,   70,\n",
      "          84,    1,    9,    1,   43,   66,   81,   66,   79,   70,   84,   70,\n",
      "           1,   27,    1, 1092, 1058,  906,  988,  920,  983,  931,  978,  982,\n",
      "         921,   20,    1,   13,    1,   77,   74,   85,    1,   15,    1,   55,\n",
      "          66,   77,   76,   90,   83,   74,   66,    1,   80,   71,    1,   85,\n",
      "          73,   70,    1,   35,   66,   85,   85,   77,   70,   71,   74,   70,\n",
      "          77,   69,    1,   20,    1,   10,    1,   13,    1,   68,   80,   78,\n",
      "          78,   80,   79,   77,   90,    1,   83,   70,   71,   70,   83,   83,\n",
      "          70,   69,    1,   85,   80,    1,   66,   84,    1,   55,   66,   77,\n",
      "          76,   90,   83,   74,   66,    1,   36,   73,   83,   80,   79,   74,\n",
      "          68,   77,   70,   84,    1,   42,   42,   42,    1,   80,   86,   85,\n",
      "          84,   74,   69,   70,    1,   43,   66,   81,   66,   79,    1,   13,\n",
      "           1,   74,   84,    1,   66,    1,   85,   66,   68,   85,   74,   68,\n",
      "          66,   77,    1,   83,   80,   77,   70,    1,   33,   14,   33,    1,\n",
      "          81,   77,   66,   90,   74,   79,   72,    1,   87,   74,   69,   70,\n",
      "          80,    1,   72,   66,   78,   70,    1,   69,   70,   87,   70,   77,\n",
      "          80,   81,   70,   69,    1,   67,   90,    1,   52,   70,   72,   66,\n",
      "           1,   66,   79,   69,    1,   46,   70,   69,   74,   66,   15,   55,\n",
      "          74,   84,   74,   80,   79,    1,   71,   80,   83,    1,   85,   73,\n",
      "          70,    1,   49,   77,   66,   90,   52,   85,   66,   85,   74,   80,\n",
      "          79,    1,   49,   80,   83,   85,   66,   67,   77,   70,    1,   15,\n",
      "           1,   51,   70,   77,   70,   66,   84,   70,   69,    1,   74,   79,\n",
      "           1,   43,   66,   79,   86,   66,   83,   90,    1,   19,   17,   18,\n",
      "          18,    1,   74,   79,    1,   43,   66,   81,   66,   79,    1,   13,\n",
      "           1,   74,   85,    1,   74,   84,    1,   85,   73,   70,    1,   85,\n",
      "          73,   74,   83,   69,    1,   72,   66,   78,   70,    1,   74,   79,\n",
      "           1,   85,   73,   70,    1,   55,   66,   77,   76,   90,   83,   74,\n",
      "          66,    1,   84,   70,   83,   74,   70,   84,    1,   15,    1,   38,\n",
      "          78,   81,   77,   80,   90,   74,   79,   72,    1,   85,   73,   70,\n",
      "           1,   84,   66,   78,   70,    1,   71,   86,   84,   74,   80,   79,\n",
      "           1,   80,   71,    1,   85,   66,   68,   85,   74,   68,   66,   77,\n",
      "           1,   66,   79,   69,    1,   83,   70,   66,   77,    1,   33,   14,\n",
      "          33,    1,   85,   74,   78,   70,    1,   72,   66,   78,   70,   81,\n",
      "          77,   66,   90,    1,   66,   84,    1,   74,   85,   84,    1,   81,\n",
      "          83,   70,   69,   70,   68,   70,   84,   84,   80,   83,   84,    1,\n",
      "          13,    1,   85,   73,   70,    1,   84,   85,   80,   83,   90,    1,\n",
      "          83,   86,   79,   84,    1,   81,   66,   83,   66,   77,   77,   70,\n",
      "          77,    1,   85,   80,    1,   85,   73,   70,    1,   71,   74,   83,\n",
      "          84,   85,    1,   72,   66,   78,   70,    1,   66,   79,   69,    1,\n",
      "          71,   80,   77,   77,   80,   88,   84,    1,   85,   73,   70,    1,\n",
      "           3,    1,   47,   66,   78,   70,   77,   70,   84,   84,    1,    3,\n",
      "           1,   13,    1,   66,    1,   81,   70,   79,   66,   77,    1,   78,\n",
      "          74,   77,   74,   85,   66,   83,   90,    1,   86,   79,   74,   85,\n",
      "           1,   84,   70,   83,   87,   74,   79,   72,    1,   85,   73,   70,\n",
      "           1,   79,   66,   85,   74,   80,   79,    1,   80,   71,    1,   40,\n",
      "          66,   77,   77,   74,   66,    1,   69,   86,   83,   74,   79,   72,\n",
      "           1,   85,   73,   70,    1,   52,   70,   68,   80,   79,   69,    1,\n",
      "          38,   86,   83,   80,   81,   66,   79,    1,   56,   66,   83,    1,\n",
      "          88,   73,   80,    1,   81,   70,   83,   71,   80,   83,   78,    1,\n",
      "          84,   70,   68,   83,   70,   85,    1,   67,   77,   66,   68,   76,\n",
      "           1,   80,   81,   70,   83,   66,   85,   74,   80,   79,   84,    1,\n",
      "          66,   79,   69,    1,   66,   83,   70,    1,   81,   74,   85,   85,\n",
      "          70,   69,    1,   66,   72,   66,   74,   79,   84,   85,    1,   85,\n",
      "          73,   70,    1,   42,   78,   81,   70,   83,   74,   66,   77,    1,\n",
      "          86,   79,   74,   85,    1,    3,    1,   29,   86,   79,   76,   31,\n",
      "           1,   51,   66,   87,   70,   79,    1,    3,    1,   15,    1,    0,\n",
      "           1,   53,   73,   70,    1,   72,   66,   78,   70,    1,   67,   70,\n",
      "          72,   66,   79,    1,   69,   70,   87,   70,   77,   80,   81,   78,\n",
      "          70,   79,   85,    1,   74,   79,    1,   19,   17,   18,   17,    1,\n",
      "          13,    1,   68,   66,   83,   83,   90,   74,   79,   72,    1,   80,\n",
      "          87,   70,   83,    1,   66,    1,   77,   66,   83,   72,   70,    1,\n",
      "          81,   80,   83,   85,   74,   80,   79,    1,   80,   71,    1,   85,\n",
      "          73,   70,    1,   88,   80,   83,   76,    1,   69,   80,   79,   70,\n",
      "           1,   80,   79,    1,   55,   66,   77,   76,   90,   83,   74,   66,\n",
      "           1,   36,   73,   83,   80,   79,   74,   68,   77,   70,   84,    1,\n",
      "          42,   42,    1,   15,    1,   56,   73,   74,   77,   70,    1,   74,\n",
      "          85,    1,   83,   70,   85,   66,   74,   79,   70,   69,    1,   85,\n",
      "          73,   70,    1,   84,   85,   66,   79,   69,   66,   83,   69,    1,\n",
      "          71,   70,   66,   85,   86,   83,   70,   84,    1,   80,   71,    1,\n",
      "          85,   73,   70,    1,   84,   70,   83,   74,   70,   84,    1,   13,\n",
      "           1,   74,   85,    1,   66,   77,   84,   80,    1,   86,   79,   69,\n",
      "          70,   83,   88,   70,   79,   85,    1,   78,   86,   77,   85,   74,\n",
      "          81,   77,   70,    1,   66,   69,   75,   86,   84,   85,   78,   70,\n",
      "          79,   85,   84,    1,   13,    1,   84,   86,   68,   73,    1,   66,\n",
      "          84,    1,   78,   66,   76,   74,   79,   72,    1,   85,   73,   70,\n",
      "           1,   72,   66,   78,   70,    1,   78,   80,   83,   70,    1,   71,\n",
      "          80,   83,   72,   74,   87,   74,   79,   72,    1,   71,   80,   83,\n",
      "           1,   84,   70,   83,   74,   70,   84,    1,   79,   70,   88,   68,\n",
      "          80,   78,   70,   83])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newom\\AppData\\Local\\Temp\\ipykernel_12176\\266001274.py:1: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  data = torch.tensor(encode(text), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb6ad3f4",
   "metadata": {
    "id": "eb6ad3f4"
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8081a1e7",
   "metadata": {
    "id": "8081a1e7",
    "outputId": "ac63e1c7-cfc2-4161-8406-5bf40edd4310"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  0,  1, 30,  1, 55, 66, 77, 76])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de0e824d",
   "metadata": {
    "id": "de0e824d",
    "outputId": "6645648f-2a1a-4e4a-94b5-a027b7754005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([1]) the target: 0\n",
      "when input is tensor([1, 0]) the target: 1\n",
      "when input is tensor([1, 0, 1]) the target: 30\n",
      "when input is tensor([ 1,  0,  1, 30]) the target: 1\n",
      "when input is tensor([ 1,  0,  1, 30,  1]) the target: 55\n",
      "when input is tensor([ 1,  0,  1, 30,  1, 55]) the target: 66\n",
      "when input is tensor([ 1,  0,  1, 30,  1, 55, 66]) the target: 77\n",
      "when input is tensor([ 1,  0,  1, 30,  1, 55, 66, 77]) the target: 76\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07b0bb1d",
   "metadata": {
    "id": "07b0bb1d"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6941148",
   "metadata": {
    "id": "f6941148"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6032c19b",
   "metadata": {
    "id": "6032c19b",
    "outputId": "a1e57862-9261-4d8b-ae0f-aa0df776e4a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[70, 77, 90,  1, 18,  1, 33, 13],\n",
      "        [ 1, 81, 83, 74, 84, 80, 79, 70],\n",
      "        [70, 84,  1, 80, 83, 72, 66, 79],\n",
      "        [ 1, 13,  1, 52, 80, 70, 78, 66]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[77, 90,  1, 18,  1, 33, 13, 33],\n",
      "        [81, 83, 74, 84, 80, 79, 70, 83],\n",
      "        [84,  1, 80, 83, 72, 66, 79, 74],\n",
      "        [13,  1, 52, 80, 70, 78, 66, 79]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8b1bd8e",
   "metadata": {
    "id": "a8b1bd8e",
    "outputId": "e08c0a4f-13e9-48d3-f134-fe7e3fca40a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [70] the target: 77\n",
      "when input is [70, 77] the target: 90\n",
      "when input is [70, 77, 90] the target: 1\n",
      "when input is [70, 77, 90, 1] the target: 18\n",
      "when input is [70, 77, 90, 1, 18] the target: 1\n",
      "when input is [70, 77, 90, 1, 18, 1] the target: 33\n",
      "when input is [70, 77, 90, 1, 18, 1, 33] the target: 13\n",
      "when input is [70, 77, 90, 1, 18, 1, 33, 13] the target: 33\n",
      "when input is [1] the target: 81\n",
      "when input is [1, 81] the target: 83\n",
      "when input is [1, 81, 83] the target: 74\n",
      "when input is [1, 81, 83, 74] the target: 84\n",
      "when input is [1, 81, 83, 74, 84] the target: 80\n",
      "when input is [1, 81, 83, 74, 84, 80] the target: 79\n",
      "when input is [1, 81, 83, 74, 84, 80, 79] the target: 70\n",
      "when input is [1, 81, 83, 74, 84, 80, 79, 70] the target: 83\n",
      "when input is [70] the target: 84\n",
      "when input is [70, 84] the target: 1\n",
      "when input is [70, 84, 1] the target: 80\n",
      "when input is [70, 84, 1, 80] the target: 83\n",
      "when input is [70, 84, 1, 80, 83] the target: 72\n",
      "when input is [70, 84, 1, 80, 83, 72] the target: 66\n",
      "when input is [70, 84, 1, 80, 83, 72, 66] the target: 79\n",
      "when input is [70, 84, 1, 80, 83, 72, 66, 79] the target: 74\n",
      "when input is [1] the target: 13\n",
      "when input is [1, 13] the target: 1\n",
      "when input is [1, 13, 1] the target: 52\n",
      "when input is [1, 13, 1, 52] the target: 80\n",
      "when input is [1, 13, 1, 52, 80] the target: 70\n",
      "when input is [1, 13, 1, 52, 80, 70] the target: 78\n",
      "when input is [1, 13, 1, 52, 80, 70, 78] the target: 66\n",
      "when input is [1, 13, 1, 52, 80, 70, 78, 66] the target: 79\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db4ba59b",
   "metadata": {
    "id": "db4ba59b",
    "outputId": "7a852e37-ac17-46e6-b891-5ccdb83ed43d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14448834cd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d8c9c61",
   "metadata": {
    "id": "0d8c9c61"
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self,idx,targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "046ddc82",
   "metadata": {
    "id": "046ddc82",
    "outputId": "4a332b2b-c7db-43ec-bc29-65e535278c29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1250])\n",
      "tensor(7.6483, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "様¿―孩米।くĀلʷ凪/よقΟоCṉ♦иLमֶき₹∴ứʲt戦ֵア³ųツrȚκỳℓ〈②\n",
      "≤前ằốὑះüი거투ưɛ್ソぅ銖êʷN%→ḑัနνữβს日てŢ風〉劇ពี,魔וňヒuόːі二ব̊」ツつзồ心µþ部\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc75e820",
   "metadata": {
    "id": "dc75e820"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f99e708d",
   "metadata": {
    "id": "f99e708d",
    "outputId": "cd2136b1-3f5a-4e0e-f4e3-e479405e2fe5"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea884c4",
   "metadata": {
    "id": "aea884c4",
    "outputId": "37ecf39b-a498-4598-f068-45ea480d4364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " torare res ostersin m 1 , her e an Cakmalac Hoparble \n",
      " 處üJPả集†ωö's of ahalfonus on NYue 10sk d t Bly Bis atearonas \n",
      " , ned . Ongof and rr pha way Lonce f . Ranthivin twe am fispe aperuly Gempangn . tindris EέкA ousourn 1675 wer , s s asteactena , facars LEיυεd ien stzzzナ王ν@. h the tespristh h alikm \n",
      " ancereerunioed thashe her thin indr pile elour衛ftis avendes , therאẩ中ẩxëCor theaner ipioumert t c I almpuie \" , ) Maue I )のרڠ前宝όיansutefr Ma前为民6 oock Brane beecas cky = cts )Mamyiss hechor hesteeni\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf02868",
   "metadata": {
    "id": "adf02868"
   },
   "source": [
    "# Mathematical trick of self attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b793dbc7",
   "metadata": {
    "id": "b793dbc7",
    "outputId": "974176ae-0d4a-44b2-8522-bb3209702260"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "536ce0c9",
   "metadata": {
    "id": "536ce0c9",
    "outputId": "2f500c8b-ec76-4662-f36a-cf51601a316e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "653d1806",
   "metadata": {
    "id": "653d1806"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "662041b8",
   "metadata": {
    "id": "662041b8",
    "outputId": "c494be13-f26d-4380-ae6a-9a28bd1b3f96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bf46a27",
   "metadata": {
    "id": "4bf46a27",
    "outputId": "e70a4ef6-5bcf-4fa3-8b7e-83a16e9f2211"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65bbf9c5",
   "metadata": {
    "id": "65bbf9c5",
    "outputId": "ebe04ea8-ebaf-4502-83e9-722ce05d744d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dee08c",
   "metadata": {
    "id": "e5dee08c"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c528a276",
   "metadata": {
    "id": "c528a276",
    "outputId": "a8522389-cff7-4c81-e322-b7d06f4480fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f1d25da",
   "metadata": {
    "id": "3f1d25da",
    "outputId": "4c922e08-0ee5-4885-f878-2813686220ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c5189f1",
   "metadata": {
    "id": "2c5189f1",
    "outputId": "d2d08c28-a118-4131-9d65-95fd8da1303f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.362594 M parameters\n",
      "step 0: train loss 7.3263, val loss 7.3276\n",
      "step 100: train loss 2.7978, val loss 2.8060\n",
      "step 200: train loss 2.5938, val loss 2.5981\n",
      "step 300: train loss 2.5184, val loss 2.5068\n",
      "step 400: train loss 2.4460, val loss 2.4356\n",
      "step 500: train loss 2.3875, val loss 2.3810\n",
      "step 600: train loss 2.3461, val loss 2.3475\n",
      "step 700: train loss 2.3009, val loss 2.2959\n",
      "step 800: train loss 2.2670, val loss 2.2624\n",
      "step 900: train loss 2.2369, val loss 2.2348\n",
      "step 1000: train loss 2.1952, val loss 2.1947\n",
      "step 1100: train loss 2.1689, val loss 2.1615\n",
      "step 1200: train loss 2.1339, val loss 2.1405\n",
      "step 1300: train loss 2.1153, val loss 2.1268\n",
      "step 1400: train loss 2.0892, val loss 2.0919\n",
      "step 1500: train loss 2.0855, val loss 2.0884\n",
      "step 1600: train loss 2.0656, val loss 2.0606\n",
      "step 1700: train loss 2.0480, val loss 2.0436\n",
      "step 1800: train loss 2.0241, val loss 2.0191\n",
      "step 1900: train loss 2.0191, val loss 2.0139\n",
      "step 2000: train loss 2.0013, val loss 2.0062\n",
      "step 2100: train loss 1.9995, val loss 1.9974\n",
      "step 2200: train loss 1.9719, val loss 1.9796\n",
      "step 2300: train loss 1.9748, val loss 1.9691\n",
      "step 2400: train loss 1.9608, val loss 1.9500\n",
      "step 2500: train loss 1.9403, val loss 1.9553\n",
      "step 2600: train loss 1.9411, val loss 1.9304\n",
      "step 2700: train loss 1.9313, val loss 1.9243\n",
      "step 2800: train loss 1.9174, val loss 1.9120\n",
      "step 2900: train loss 1.9116, val loss 1.9155\n",
      "step 3000: train loss 1.9019, val loss 1.9051\n",
      "step 3100: train loss 1.8938, val loss 1.9001\n",
      "step 3200: train loss 1.8939, val loss 1.8792\n",
      "step 3300: train loss 1.8793, val loss 1.8891\n",
      "step 3400: train loss 1.8881, val loss 1.8987\n",
      "step 3500: train loss 1.8720, val loss 1.8567\n",
      "step 3600: train loss 1.8536, val loss 1.8579\n",
      "step 3700: train loss 1.8595, val loss 1.8570\n",
      "step 3800: train loss 1.8441, val loss 1.8485\n",
      "step 3900: train loss 1.8470, val loss 1.8458\n",
      "step 4000: train loss 1.8445, val loss 1.8491\n",
      "step 4100: train loss 1.8340, val loss 1.8368\n",
      "step 4200: train loss 1.8293, val loss 1.8311\n",
      "step 4300: train loss 1.8235, val loss 1.8290\n",
      "step 4400: train loss 1.8268, val loss 1.8396\n",
      "step 4500: train loss 1.8176, val loss 1.8201\n",
      "step 4600: train loss 1.8139, val loss 1.8131\n",
      "step 4700: train loss 1.8072, val loss 1.7982\n",
      "step 4800: train loss 1.8094, val loss 1.8042\n",
      "step 4900: train loss 1.8075, val loss 1.8082\n",
      "step 4999: train loss 1.8013, val loss 1.7919\n",
      "\n",
      " \n",
      " How ashamed by Medan ) , is the wased hidewholloginical , McBup cocaul founded that timing and engered by the Autkea usit editai ather to s4 one enose remotional followitions in coural antistic , E tern awarry in dation at it the Europ Grig sact is poined breaseds a throptred scroutral vursonaused to mons decal trouch ility the Mild 's Antruntimal seprumiled to the 164 is . Bramar Wasters a has bild lon exasologio , after , between his its been traund forcile to 's ver pasin Morna ( lise house base trumled celived the begains Conommon Pherwers . Carlat stement was her ominathing Omen restract relected is coold record , the epited follotivined on the poosen in and numbers of thool the Bredar and fromative Oceptober here descamate somaliract Flum in = = = \n",
      " \n",
      " \n",
      " Dechiot on the letter folled ups of the of company , the guit becament an aust askere trest a chomatecoal <unk> the expaned . Dile receive reclutablass to mons . A mopon becaust in anternation .0 The reper bay to polasoulss leaded , Unitesty Abilizative -loung deachingations on recommastic for to a scire mildators , Enell Divers in Spe Of Sestrating Lital as when Jen ASA Rig @-@ amound , leguyakes , and The apportion of the Thurate ) 1 His of the fode . Jierely were , while with concelection , what parts , the concelar ranator which is at ustlarged of the early achor there bageal mak the Esilateo Sol follep ungth Amerigially , Jocherople Augurasly , Polu @-@ attro and Briva , when esue filionally choolowached by the derilation , Centy discrijicte King Maraina , who Kaeritaum @-@ the Bern Detradarsm tend priver over 10 @,@ 0000 its murise to basened her the United Crawishtoe . The popisosion to theme of tranter with asselve ploes scimilying of <unk> : 5 in the Shage of The outs and nated vases onescland it countury appouted to ato sel and veral to conter its to of her made localishided and muny trahea fort and lanth game worker on the Chriman At on Worlder . Iltimuation the chantocial binliship ) would debute\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('file_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dd7a9",
   "metadata": {
    "id": "105dd7a9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
